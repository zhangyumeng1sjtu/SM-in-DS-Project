---
title: "Step 4. Non-parametric model"
author: "Yumeng Zhang"
date: "5/27/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(caret)
library(ggplot2)
set.seed(123)
```

```{r}
load('../data/train_test_data.Rdata')
```

注：本来tree models可以直接分类factor类型的数据，但为了保持统一，选择在dummy_data上训练
随机森林设置选取的feature数为预测变量数目平方根
```{r}
mtry <- sqrt(ncol(trainData)-1)
tunegrid <- expand.grid(mtry=mtry, splitrule="gini", min.node.size=10)
model_rf <- train(HeartDisease ~ ., data = trainDataSmote,
                  method = 'ranger',  metric = "ROC",
                  tuneGrid = tunegrid, num.trees = 500,  max.depth = 6, 
                  importance = 'impurity', trControl = ctrl)
```

Gradient boosting
```{r}
tuneGrid <- expand.grid(interaction.depth=1, n.trees=100,
                        shrinkage=1, n.minobsinnode=10)
model_gbm <- train(HeartDisease ~ .,data = trainDataSmote,
                   method = 'gbm',  metric = "ROC",
                   tuneGrid=tuneGrid,  trControl = ctrl)
```

随机森林可以plot variable importance
```{r fig.height=8, fig.width=6}
imp <- varImp(model_rf)$importance
imp <- cbind(Predictor = rownames(imp), imp)
rownames(imp) <- 1:nrow(imp)
colnames(imp) <- c('Predictor', 'Importance')
p <- ggplot(imp, aes(x = Importance, y = reorder(Predictor, Importance), label = round(Importance, 2)))
p + geom_bar(stat='identity') + theme_bw() + labs(y='') + theme(panel.grid.minor = element_blank())
```

```{r}
save(model_rf, model_gbm, file = '../models/tree.models.Rdata')
```
